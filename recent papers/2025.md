### ðŸ“Œ**Recent papers (2025)**
- [VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation](https://arxiv.org/pdf/2510.09607)
- [StreamingVLM: Real-Time Understanding for Infinite Video Streams](https://arxiv.org/pdf/2510.09608)
- [Vision Language Models: A Survey of 26K Papers](https://arxiv.org/pdf/2510.09586)
- [Zero-shot image privacy classification with Vision-Language Models](https://arxiv.org/pdf/2510.09253)
- [RetouchLLM: Training-free Code-based Image Retouching with Vision Language Models](https://arxiv.org/pdf/2510.08054)
- [NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints](https://arxiv.org/pdf/2510.08565)
- [SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models](https://arxiv.org/pdf/2510.08559)
- [SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/pdf/2510.08531)
- [To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models](https://arxiv.org/pdf/2510.08510)
- [The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping](https://arxiv.org/pdf/2510.08482)
- [Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models](https://arxiv.org/pdf/2510.07135)
- [Enhancing Concept Localization in CLIP-based Concept Bottleneck Models](https://arxiv.org/pdf/2510.07115)
- [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/pdf/2510.06820)
- [TTRV: Test-Time Reinforcement Learning for Vision Language Models](https://arxiv.org/pdf/2510.06783)
- [See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models](https://arxiv.org/pdf/2510.05408)
- [Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation](https://arxiv.org/pdf/2510.06131)
- [Medical Vision Language Models as Policies for Robotic Surgery](https://arxiv.org/pdf/2510.06064)
- [Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA](https://arxiv.org/pdf/2510.06067)
- [Data Factory with Minimal Human Effort Using VLMs](https://arxiv.org/pdf/2510.05722)
- [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/pdf/2510.04477)
- [Visual Representations inside the Language Model](https://arxiv.org/pdf/2510.04819)
- [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/pdf/2510.01954)
- [Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations](https://arxiv.org/pdf/2510.00047)
- [Can World Models Benefit VLMs for World Dynamics?](https://arxiv.org/pdf/2510.00855)
- [Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs](https://arxiv.org/pdf/2510.00705)
- [SeMoBridge: Semantic Modality Bridge for Efficient Few-Shot Adaptation of CLIP](https://arxiv.org/pdf/2509.26036)
- [Interpret, prune and distill Donut : towards lightweight VLMs for VQA on document](https://arxiv.org/pdf/2509.26235)
- [Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection](https://arxiv.org/pdf/2509.23236)
- [LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models](https://arxiv.org/pdf/2509.23729)
- [HIVTP: A Training-Free Method to Improve VLMs Efficiency via Hierarchical Visual Token Pruning Using Middle-Layer-Based Importance Score](https://arxiv.org/pdf/2509.23663)
- [RIV: Recursive Introspection Mask Diffusion Vision Language Model](https://arxiv.org/pdf/2509.23625)
- [TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models](https://arxiv.org/pdf/2509.24566)
- [Visual Jigsaw Post-Training Improves MLLMs](https://arxiv.org/pdf/2509.25190)
- [GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts](https://arxiv.org/pdf/2509.25160)
- [TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models](https://arxiv.org/pdf/2509.25143)
- [VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning](https://arxiv.org/pdf/2509.25033)
- [CoFFT: Chain of Foresight-Focus Thought for Visual Language Models](https://arxiv.org/pdf/2509.22010)
- [Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors](https://arxiv.org/pdf/2509.21997)
- [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/pdf/2509.21991)
- [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/pdf/2509.21984)
- [Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models](https://arxiv.org/pdf/2509.21979)
- [Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models](https://arxiv.org/pdf/2509.22221)
- [MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models](https://arxiv.org/pdf/2509.22151)
- [RAU: Reference-based Anatomical Understanding with Vision Language Models](https://arxiv.org/pdf/2509.22404)
- [Calibration-Aware Prompt Learning for Medical Vision-Language Models](https://arxiv.org/pdf/2509.15226)
- [EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence](https://arxiv.org/pdf/2509.14977)
- [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/pdf/2509.13919)
- [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/pdf/2509.13836)
- [3D Aware Region Prompted Vision Language Model](https://arxiv.org/pdf/2509.13317)
- [More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era](https://arxiv.org/pdf/2509.13175)
- [HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models](https://arxiv.org/pdf/2509.13067)
- [Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models](https://arxiv.org/pdf/2509.13031)
- [Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models](https://arxiv.org/pdf/2509.12897)
- [Open-ended Hierarchical Streaming Video Understanding with Vision Language Models](https://arxiv.org/pdf/2509.12145)
- [Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models](https://arxiv.org/pdf/2509.12132)
- [Lost in Embeddings: Information Loss in Vision-Language Models](https://arxiv.org/pdf/2509.11986)
- [SpecVLM: Fast Speculative Decoding in Vision-Language Models](https://arxiv.org/pdf/2509.11815)
- [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/pdf/2509.10345)
- [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/pdf/2509.10278)
- [VARCO-VISION-2.0 Technical Report](https://arxiv.org/pdf/2509.10105)
- [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/pdf/2509.10026)
- [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/pdf/2509.09190)
- [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/pdf/2509.09159)
- [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/pdf/2509.08777)
- [Visual Representation Alignment for Multimodal Large Language Models](https://arxiv.org/pdf/2509.07979)
- [Fine-Tuning Vision-Language Models for Visual Navigation Assistance](https://arxiv.org/pdf/2509.07488)
- [Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning](https://arxiv.org/pdf/2509.06461)
- [Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework](https://arxiv.org/pdf/2509.05000)
- [Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture](https://arxiv.org/pdf/2509.02359)
- [How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images](https://arxiv.org/pdf/2508.21565)
- [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/pdf/2508.20830)
- [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/pdf/2508.20783)
- [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/pdf/2508.20758)
- [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/pdf/2508.20691)
- [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/pdf/2508.20655)
- [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/pdf/2508.19972)
- [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/pdf/2508.19652)
- [DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding](https://arxiv.org/pdf/2508.15297)
- [Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images](https://arxiv.org/pdf/2508.15256)
- [WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion](https://arxiv.org/pdf/2508.14537)
- [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/pdf/2508.13142)
- [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/pdf/2508.12877)
- [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/pdf/2508.12861)
- [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/pdf/2508.11628)
- [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/pdf/2508.11350)
- [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/pdf/2508.11277)
- [Vision-Language Models display a strong gender bias](https://arxiv.org/pdf/2508.11262)
- [From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models](https://arxiv.org/pdf/2508.10770)
- [IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning](https://arxiv.org/pdf/2508.10681)
- [AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models](https://arxiv.org/pdf/2508.10667)
- [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/pdf/2508.10637)
- [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/pdf/2508.09818)
- [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/pdf/2508.09779)
- [3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs](https://arxiv.org/pdf/2508.08821)
- [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/pdf/2508.08199)
- [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/pdf/2508.07996)
- [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/pdf/2508.07871)
- [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/pdf/2508.06259)
- [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/pdf/2508.06202)
- [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/pdf/2508.06142)
- [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/pdf/2508.05502)
- [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/pdf/2508.04469)
- [Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion](https://arxiv.org/pdf/2508.04453)
- [IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models](https://arxiv.org/pdf/2508.03469)
- [Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models](https://arxiv.org/pdf/2508.02671)
- [Engagement Prediction of Short Videos with Large Multimodal Models](https://arxiv.org/pdf/2508.02516)
- [SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with Vision Foundation Models](https://arxiv.org/pdf/2508.02464)
- [Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens](https://arxiv.org/pdf/2508.02419)
- [CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions](https://arxiv.org/pdf/2508.02329)
- [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/pdf/2508.00553)
- [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/pdf/2508.00549)
- [See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs](https://arxiv.org/pdf/2507.22003)
- [Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards](https://arxiv.org/pdf/2507.21745)
- [Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM](https://arxiv.org/pdf/2507.20994)
- [DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception](https://arxiv.org/pdf/2507.20879)
- [METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/pdf/2507.20842)
- [TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model](https://arxiv.org/pdf/2507.20630)
- [EA-ViT: Efficient Adaptation for Elastic Vision Transformer](https://arxiv.org/pdf/2507.19360)
- [SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality](https://arxiv.org/pdf/2507.19264)
- [MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective](https://arxiv.org/pdf/2507.19131)
- [LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/pdf/2507.19110)
- [Negation-Aware Test-Time Adaptation for Vision-Language Models](https://arxiv.org/pdf/2507.19064)
- [BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems](https://arxiv.org/pdf/2507.17722)
- [Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls](https://arxiv.org/pdf/2507.17467)
- [Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation](https://arxiv.org/pdf/2507.16716)
- [Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models](https://arxiv.org/pdf/2507.16257)
- [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/pdf/2507.15807)
- [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/pdf/2507.15652)
- [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/pdf/2507.15542)
- [One Last Attention for Your Vision-Language Model](https://arxiv.org/pdf/2507.15480)
- [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/pdf/2507.14067)
- [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/pdf/2507.13868)
- [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/pdf/2507.13773)
- [VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/pdf/2507.13348)
- [SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models](https://arxiv.org/pdf/2507.13152)
- [GLAD: Generalizable Tuning for Vision-Language Models](https://arxiv.org/pdf/2507.13089)
- [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/pdf/2507.12236)
- [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/pdf/2507.12232)
- [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/pdf/2507.11200)
- [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/pdf/2507.11153)
- [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/pdf/2507.10355)
- [BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis](https://arxiv.org/pdf/2507.08607)
- [Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2507.08410)
- [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2507.05677)
- [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning](https://arxiv.org/pdf/2507.05255)
- [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/pdf/2507.02844)
- [Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration](https://arxiv.org/pdf/2506.21509)
- [ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models](https://arxiv.org/pdf/2506.21356)
- [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/pdf/2506.20168)
- [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/pdf/2506.20155)
- [PEVLM: Parallel Encoding for Vision-Language Models](https://arxiv.org/pdf/2506.19651)
- [Visual hallucination detection in large vision-language models via evidential conflict](https://arxiv.org/pdf/2506.19513)
- [Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning](https://arxiv.org/pdf/2506.19469)
- [Emergence of Text Readability in Vision Language Models](https://arxiv.org/pdf/2506.19389)
- [Universal Video Temporal Grounding with Generative Multi-modal Large Language Models](https://arxiv.org/pdf/2506.18883)
- [VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning](https://arxiv.org/pdf/2506.18564)
- [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/pdf/2506.18504)
- [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.17221)
- [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/pdf/2506.17218)
- [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/pdf/2506.17144)
- [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/pdf/2506.16962)
- [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/pdf/2506.16826)
- [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](https://arxiv.org/pdf/2506.16806)
- [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/pdf/2506.16796)
- [Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning](https://arxiv.org/pdf/2506.15649)
- [Demystifying the Visual Quality Paradox in Multimodal Large Language Models](https://arxiv.org/pdf/2506.15645)
- [OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models](https://arxiv.org/pdf/2506.15318)
- [Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models](https://arxiv.org/pdf/2506.15201)
- [PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning](https://arxiv.org/pdf/2506.14907)
- [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/pdf/2506.14766)
- [Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models](https://arxiv.org/pdf/2506.14674)
- [SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks](https://arxiv.org/pdf/2506.14512)
- [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/pdf/2506.14473)
- [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/pdf/2506.14451)
- [AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.13757)
- [OTFusion: Bridging Vision-only and Vision-Language Models via Optimal Transport for Transductive Zero-Shot Learning](https://arxiv.org/pdf/2506.13723)
- [DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models](https://arxiv.org/pdf/2506.13638)
- [MambaMia: A State-Space-Model-Based Compression for Efficient Video Understanding in Large Multimodal Models](https://arxiv.org/pdf/2506.13564)
- [Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images](https://arxiv.org/pdf/2506.13458)
- [Anomaly Object Segmentation with Vision-Language Models for Steel Scrap Recycling](https://arxiv.org/pdf/2506.13282)
- [VGR: Visual Grounded Reasoning](https://arxiv.org/pdf/2506.11991)
- [How Visual Representations Map to Language Feature Space in Multimodal LLMs](https://arxiv.org/pdf/2506.11976)
- [Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning](https://arxiv.org/pdf/2506.11672)
- [EasyARC: Evaluating Vision Language Models on True Visual Reasoning](https://arxiv.org/pdf/2506.11595)
- [VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?](https://arxiv.org/pdf/2506.11571)
- [Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs](https://arxiv.org/pdf/2506.11515)
- [On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving](https://arxiv.org/pdf/2506.11472)
- [Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/pdf/2506.10967)
- [IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain](https://arxiv.org/pdf/2506.10730)
- [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/pdf/2506.09965)
- [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/pdf/2506.09040)
- [Vision Transformers Don't Need Trained Registers](https://arxiv.org/pdf/2506.08010)
- [CoMemo: LVLMs Need Image Context with Image Memory](https://arxiv.org/pdf/2506.06279)
- [Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models](https://arxiv.org/pdf/2506.06242)
- [Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study](https://arxiv.org/pdf/2506.06232)
- [Full Conformal Adaptation of Medical Vision-Language Models](https://arxiv.org/pdf/2506.06076)
- [SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs](https://arxiv.org/pdf/2506.05344)
- [Refer to Anything with Vision-Language Prompts](https://arxiv.org/pdf/2506.05342)
- [VideoMolmo: Spatio-Temporal Grounding Meets Pointing](https://arxiv.org/pdf/2506.05336)
- [MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning]()
- [Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets 3D VLMs](https://arxiv.org/pdf/2506.05318)
- [MARBLE: Material Recomposition and Blending in CLIP-Space](https://arxiv.org/pdf/2506.05313)
- [Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos](https://arxiv.org/pdf/2506.05302)
- [Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models](https://arxiv.org/pdf/2506.04220)
- [Language-Image Alignment with Fixed Text Encoders](https://arxiv.org/pdf/2506.04209)
- [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/pdf/2506.04039)
- [Vocabulary-free few-shot learning for Vision-Language Models](https://arxiv.org/pdf/2506.04005)
- [UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/pdf/2506.03147)
- [Targeted Forgetting of Image Subgroups in CLIP Models](https://arxiv.org/pdf/2506.03117)
- [Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning](https://arxiv.org/pdf/2506.03110)
- [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/pdf/2506.03097)
- [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/pdf/2506.03096)
- [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/pdf/2506.03096)
- [Dual-Process Image Generation](https://arxiv.org/pdf/2506.01955)
- [MLLMs Need 3D-Aware Representation Supervision for Scene Understanding](https://arxiv.org/pdf/2506.01946)
- [MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs](https://arxiv.org/pdf/2506.01850)
- [R2SM: Referring and Reasoning for Selective Masks](https://arxiv.org/pdf/2506.01795)
- [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/pdf/2505.24876)
- [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/pdf/2505.24872)
- [Time Blindness: Why Video-Language Models Can't See What Humans Can?](https://arxiv.org/pdf/2505.24867)
- [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/pdf/2505.24840)
- [CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning](https://arxiv.org/pdf/2505.24816)
- [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/pdf/2505.24718)
- [Conformal Prediction for Zero-Shot Models](https://arxiv.org/pdf/2505.24693)
- [Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought](https://arxiv.org/pdf/2505.23766)
- [Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models](https://arxiv.org/pdf/2505.23757)
- [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/pdf/2505.23747)
- [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/pdf/2505.23745)
- [PIXELTHINK:Towards Efficient Chain-of-Pixel Reasoning](https://arxiv.org/pdf/2505.23727)
- [DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers](https://arxiv.org/pdf/2505.23694)
- [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/pdf/2505.23693)
- [Grounded Reinforcement Learning for Visual Reasoning](https://arxiv.org/pdf/2505.23678)
- [Zero-Shot Vision Encoder Grafting via LLM Surrogates](https://arxiv.org/pdf/2505.22664)
- [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/pdf/2505.22654)
- [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.22651)
- [SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning](https://arxiv.org/pdf/2505.22596)
- [Thinking with Generated Images](https://arxiv.org/pdf/2505.22525)
- [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/pdf/2505.21500)
- [Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment](https://arxiv.org/pdf/2505.21494)
- [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/pdf/2505.21472)
- [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/pdf/2505.21465)
- [Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO](https://arxiv.org/pdf/2505.21457)
- [GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution](https://arxiv.org/pdf/2505.21375)
- [Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?](https://arxiv.org/pdf/2505.21374)
- [VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection](https://arxiv.org/pdf/2505.20289)
- [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/pdf/2505.20279)
- [Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning](https://arxiv.org/pdf/2505.20272)
- [Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models](https://arxiv.org/pdf/2505.20236)
- [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/pdf/2505.20152)
- [TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos](https://arxiv.org/pdf/2505.20124)
- [TokBench: Evaluating Your Visual Tokenizer before Visual Generation](https://arxiv.org/pdf/2505.18142)
- [Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking](https://arxiv.org/pdf/2505.18111)
- [TokBench: Evaluating Your Visual Tokenizer before Visual Generation](https://arxiv.org/pdf/2505.18142)
- [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/pdf/2505.18129)
- [Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking](https://arxiv.org/pdf/2505.18111)
- [FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation](https://arxiv.org/pdf/2505.18053)
- [LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision](https://arxiv.org/pdf/2505.18051)
- [Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation](https://arxiv.org/pdf/2505.18039)
- [Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling](https://arxiv.org/pdf/2505.17982)
- [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/pdf/2505.17022)
- [SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward](https://arxiv.org/pdf/2505.17018)
- [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/pdf/2505.17015)
- [SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding](https://arxiv.org/pdf/2505.17012)
- [SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving](https://arxiv.org/pdf/2505.16805)
- [Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation](https://arxiv.org/pdf/2505.16763)
- [STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs](https://arxiv.org/pdf/2505.15804)
- [Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models](https://arxiv.org/pdf/2505.15576)
- [Clapper: Compact Learning and Video Representation in VLMs](https://arxiv.org/pdf/2505.15529)
- [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/pdf/2505.15510)
- [Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts](https://arxiv.org/pdf/2505.15506)
- [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/pdf/2505.15489)
- [ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning](https://arxiv.org/pdf/2505.15447)
- [Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL](https://arxiv.org/pdf/2505.15436)
- [TimeCausality: Evaluating the Causal Ability in Time Dimension for Vision Language Models](https://arxiv.org/pdf/2505.15435)
- [Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks](https://arxiv.org/pdf/2505.15414)
- [Visual Question Answering on Multiple Remote Sensing Image Modalities](https://arxiv.org/pdf/2505.15401)
- [Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/pdf/2505.14683)
- [Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning](https://arxiv.org/pdf/2505.14677)
- [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/pdf/2505.14654)
- [Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency](https://arxiv.org/pdf/2505.14405)
- [DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning](https://arxiv.org/pdf/2505.14362)
- [Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives](https://arxiv.org/pdf/2505.14361)
- [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/pdf/2505.14260)
- [Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models](https://arxiv.org/pdf/2505.14257)
- [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/pdf/2505.14246)
- [UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning](https://arxiv.org/pdf/2505.14231)
- [G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning](https://arxiv.org/pdf/2505.13426)
- [From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection](https://arxiv.org/pdf/2505.13233)
- [Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption](https://arxiv.org/pdf/2505.12912)
- [End-to-End Vision Tokenizer Tuning](https://arxiv.org/pdf/2505.10562)
- [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/pdf/2505.10557)
- [Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis](https://arxiv.org/pdf/2505.10541)
- [UniEval: Unified Holistic Evaluation for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.10483)
- [Vision language models have difficulty recognizing virtual objects](https://arxiv.org/pdf/2505.10453)
- [MSCI: Addressing CLIP's Inherent Limitations for Compositional Zero-Shot Learning](https://arxiv.org/pdf/2505.10289)
- [Variational Visual Question Answering](https://arxiv.org/pdf/2505.09591)
- [Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput](https://arxiv.org/pdf/2505.09498)
- [A 2D Semantic-Aware Position Encoding for Vision Transformers](https://arxiv.org/pdf/2505.09466)
- [Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records](https://arxiv.org/pdf/2505.09435)
- [MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment](https://arxiv.org/pdf/2505.09372)
- [Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving](https://arxiv.org/pdf/2505.08725)
- [TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series](https://arxiv.org/pdf/2505.08723)
- [OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning](https://arxiv.org/pdf/2505.08617)
- [Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models](https://arxiv.org/pdf/2505.07690)
- [MAIS: Memory-Attention for Interactive Segmentation](https://arxiv.org/pdf/2505.07511)
- [Register and CLS tokens yield a decoupling of local and global features in large ViTs](https://arxiv.org/pdf/2505.05892)
- [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/pdf/2505.03181)
- [MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks](https://arxiv.org/pdf/2505.06152)
- [Does CLIP perceive art the same way we do?](https://arxiv.org/pdf/2505.05229)
- [Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects](https://arxiv.org/pdf/2505.05318)
- [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/pdf/2505.05422)
- [Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding](https://arxiv.org/pdf/2505.05446)
- [DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception](https://arxiv.org/pdf/2505.04410)
- [OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning](https://arxiv.org/pdf/2505.04601)
- [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/pdf/2505.03654)
- [Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design](https://arxiv.org/pdf/2505.00134)
- [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/pdf/2505.00150)
- [V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving](https://arxiv.org/pdf/2505.00156)
- [AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care](https://arxiv.org/pdf/2505.00275)
- [Visual Test-time Scaling for GUI Agent Grounding](https://arxiv.org/pdf/2505.00684)
- [Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection](https://arxiv.org/pdf/2504.21344)
- [Rethinking Visual Layer Selection in Multimodal LLMs](https://arxiv.org/pdf/2504.21447)
- [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/pdf/2504.21559)
- [Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization](https://arxiv.org/pdf/2504.21831)
- [DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes](https://arxiv.org/pdf/2504.20303)
- [MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation](https://arxiv.org/pdf/2504.20343)
- [Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception](https://arxiv.org/pdf/2504.20468)
- [SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/pdf/2504.20648)
- [FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models](https://arxiv.org/pdf/2504.20860)
- [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/pdf/2504.20998)
- [Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model](https://arxiv.org/pdf/2504.19739)
- [Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning](https://arxiv.org/pdf/2504.17996)
- [A Large Vision-Language Model based Environment Perception System for Visually Impaired People](https://arxiv.org/pdf/2504.18027)
- [ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding](https://arxiv.org/pdf/2504.18152)
- [E-InMeMo: Enhanced Prompting for Visual In-Context Learning](https://arxiv.org/pdf/2504.18158)
- [Revisiting Data Auditing in Large Vision-Language Models](https://arxiv.org/pdf/2504.18349)
- [Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization](https://arxiv.org/pdf/2504.18397)
- [Vision language models are unreliable at trivial spatial cognition](https://arxiv.org/pdf/2504.16061)
- [MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention](https://arxiv.org/pdf/2504.16083)
- [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/pdf/2504.14848)
- [Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation](https://arxiv.org/pdf/2504.14988)
- [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/pdf/2504.15280)
- [Perception Encoder: The best visual embeddings are not at the output of the network](https://arxiv.org/pdf/2504.13181)
- [Vision and Language Integration for Domain Generalization](https://arxiv.org/pdf/2504.12966)
- [Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training](https://arxiv.org/pdf/2504.13123)
- [Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling](https://arxiv.org/pdf/2504.13169)
- [Logits DeConfusion with CLIP for Few-Shot Learning](https://arxiv.org/pdf/2504.12104)
- [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models](https://arxiv.org/pdf/2504.12137)
- [Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR](https://arxiv.org/pdf/2504.11101)
- [SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL](https://arxiv.org/pdf/2504.11455)
- [The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer](https://arxiv.org/pdf/2504.10462)
- [SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model](https://arxiv.org/pdf/2504.10320)
- [Investigating Vision-Language Model for Point Cloud-based Vehicle Classification](https://arxiv.org/pdf/2504.08154)
- [EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models](https://arxiv.org/pdf/2504.08205)
- [VL-UR: Vision-Language-guided Universal Restoration of Images Degraded by Adverse Weather Conditions](https://arxiv.org/pdf/2504.08219)
- [VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering](https://arxiv.org/pdf/2504.08269)
- [Steering CLIP's vision transformer with sparse autoencoders](https://arxiv.org/pdf/2504.08729)
- [Taxonomy-Aware Evaluation of Vision-Language Models](https://arxiv.org/pdf/2504.05457)
- [A Lightweight Large Vision-language Model for Multimodal Medical Images](https://arxiv.org/pdf/2504.05575)
- [Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis](https://arxiv.org/pdf/2503.22420)
- [Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization](https://arxiv.org/pdf/2503.22577)
- [Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation](https://arxiv.org/pdf/2503.24368)
- [POPEN: Preference-Based Optimization and Ensemble for LVLM-Based Reasoning Segmentation](https://arxiv.org/pdf/2504.00640)
- [QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA](https://arxiv.org/pdf/2504.00654)
- [AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization](https://arxiv.org/pdf/2504.01735)
- [Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images](https://arxiv.org/pdf/2504.01838)
- [FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs](https://arxiv.org/pdf/2504.01916)
- [Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence](https://arxiv.org/pdf/2504.02799)
- [Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models](https://arxiv.org/pdf/2504.02821)
- [STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection](https://arxiv.org/pdf/2504.02823)
- [Itâ€™s a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data](https://arxiv.org/pdf/2503.24129)
- [Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation](https://arxiv.org/pdf/2503.19647v1)  
- [ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts](https://arxiv.org/pdf/2504.00691)
- [Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models](https://arxiv.org/pdf/2503.17142)  
- [Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models](https://arxiv.org/pdf/2503.17349)  
- [CoMP: Continual Multimodal Pre-training for Vision Foundation Models](https://arxiv.org/pdf/2503.18931)  
- [ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation](https://arxiv.org/pdf/2503.19755)  
- [PAVE: Patching and Adapting Video Large Language Models](https://arxiv.org/pdf/2503.19794)  
- [FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model](https://arxiv.org/pdf/2503.19839)  
- [Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding](https://arxiv.org/pdf/2503.203)  
- [Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models](https://arxiv.org/pdf/2503.20492)  
- [IAP: Improving Continual Learning of Vision-Language Models via Instance-Aware Prompting](https://arxiv.org/pdf/2503.20612)  
- [BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](https://arxiv.org/pdf/2503.21483)  
- [Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck](https://arxiv.org/pdf/2503.21757)  
- [OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement](https://arxiv.org/pdf/2503.17352)  
- [Scaling Vision Pre-Training to 4K Resolution](https://arxiv.org/pdf/2503.19903)  
- [CAFe: Unifying Representation and Generation with Contrastive-Autoregressive Finetuning](https://arxiv.org/pdf/2503.19900)
- [DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models](https://arxiv.org/pdf/2503.13443)
- [EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models](https://arxiv.org/pdf/2503.15369)
- [JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse](https://arxiv.org/pdf/2503.16365)
- [MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling](https://arxiv.org/pdf/2503.13440)
- [TULIP: Towards Unified Language-Image Pretraining](https://arxiv.org/pdf/2503.15485)
- [VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search](https://arxiv.org/pdf/2503.10582)
- [Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages](https://arxiv.org/pdf/2503.11609)  
- [Visual-RFT: Visual Reinforcement Fine-Tuning](https://arxiv.org/pdf/2503.01785)  
- [Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models](https://arxiv.org/pdf/2502.13836)  
- [Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)  
- [Memory-Augmented Latent Transformers for Any-Length Video Generation](https://www.arxiv.org/abs/2502.12632)  
- [ Visual Attention Sink In Large Multimodal Models](https://arxiv.org/pdf/2503.03321)  
- [Unified Reward Model for Multimodal Understanding and Generation](https://arxiv.org/pdf/2503.05236)  
- [Should VLMs Be Pre-trained With Image Data?](https://arxiv.org/pdf/2503.07603)    
- [Pre-Instruction Data Selection for Visual Instruction Tuning](https://arxiv.org/pdf/2503.07591)  
- [VisRL: Intention-Driven Visual Perception via Reinforced Reasoning](https://arxiv.org/pdf/2503.07523)  
- [Interpolating Between Autoregressive and Diffusion Language Models](https://arxiv.org/pdf/2503.09573)  
- [PLLaVA: Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](https://arxiv.org/pdf/2404.16994)  
- [TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/pdf/2404.12803)  
- [Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning](https://arxiv.org/pdf/2402.11690)  
- [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/pdf/2401.15947)  
- [What matters when building vision-language models?](https://arxiv.org/pdf/2405.02246)  
- [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/pdf/2401.13601)  
- [VisionZip: Longer is Better but Not Necessary in Vision Language Models](https://arxiv.org/pdf/2412.04467)  
- [Encoder-Free Vision-Language Models](https://arxiv.org/pdf/2406.11832)  
